1a) It is better to choose a random value for the initial sequence number. There is a maximum segment lifetime for packets on the network, after which time it is assumed that the packet is no longer on the network. A problem arises, however, if a connection is established between ADR_A:PORT_X and ADR_B:PORT_Y, 1 is always chosen as the initial sequence number, data is transmitted, and then the connection ends for some reason. If ADR_A:PORT_X and ADR_B:PORT_Y establish another connection within maximum segment lifetime period of time (or as an even more extreme case, very soon after closing the initial connection), then it is possible for a packet sent from the initial connection to be received during the second connection and be misinterpreted. Because there are no restrictions on the time interval between successive connections between the same address/port pairs, it is important to be careful when choosing an initial sequence number. To be clear, choosing a random initial sequence number does not guarantee that this misinterpretation of a packet doesn't occur, but it at least reduces the probability of it occurring.

1b) There are two variants of this problem, one in which the attacker sends all of the SYN packets from the same node:port pair and one in which the attacker either sends SYN packets from different ports on his true node or spoofs many different node:port pairs. 

In the first scenario, my listen socket would create a new TCP socket for the node:port pair and place it in the request queue for the listen socket. Then, TransferServer (or whatever application was using the server socket) would soon after call accept, creating a TransferWorker (or a generic worker thread in the general case), who would wait for data/a FIN packet. Instead of receiving either, it would receive many SYN packets, each of which would be responded to with the original ACK packet that was sent. The problem in this scenario would be that all of the SYN packets could impede throughput of the socket (its ability to handle and respond to legitimate requests). In this scenario, one solution would be to have a socket upon receiving some threshold number of SYN packets to register the node:port it is connected to with TCPManager to drop future packets received from this node:port pair (which would be assumed to be an attacker). Then, the socket could close the connection. Then, upon receiving additional SYN packets, TCPManager could just drop them.

The more dangerous scenario is the second one, in which the attacker sends all of the SYN packets from different node:port pairs. As a result, my implementation of TCP would open a new connection for each request, place each one in the request queue for the listening socket (after a short period of time, it would be removed through accept()), and each new socket would wait (indefinitely) for data/a FIN packet. This would be problematic for two reasons: first, the listening socket backlog would always be near capacity, meaning many new legitimate connections could be rejected. Second, the number of open connections would increase without bound. One method for preventing this would be to associate a timeout with each connection -- if data isn't received after some number of seconds to close the connection. An even better method would be to ensure that TCP doesn't have to maintain state until the sender's node:port pair has been verified. One way to do this would be to have the sender initially send a "request to connect" to server with a given node:port pair. The server could then encrypt this with a private key and send that encrypted node:port pair back to the receiver. The receiver could then connect with this encrypted value, and only then would the server create a new socket for the sender (after the identity is verified). If there is a worry about a sender creating too many legitimate node:port pairs for the server to handle, the number of ports that a particular node could connect to the server through could be capped.

1c) The connection will remain open (and in fact never be closed). This is also a problem if the sender sends a FIN packet and that FIN packet is lost since the sender immediately closes the connection after sending the FIN packet. The most straightforward way to prevent this attack would be to implement a timeout mechanism (like we did for our asynchronous servers in assignment 3), such that if the TCP socket doesn't receive data for a certain period of time, it will send a FIN packet and close itself. This could be done in a similar manner to how timeouts are implemented for lost packets (only it would be for a longer period of time) -- in other words, timeouts could be registered with Manager when new data is received, and if no new data is received once the timeout occurs, the socket could send a FIN packet and close itself.
